{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM4sOeD+nBtgVXhvORRM9el",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MiguelAngel-ht/Machiine_Learning_Models/blob/main/CHAT_BOT_Training_with_Sequential_Algorithm_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HwMxZ6ePwF4R"
      },
      "outputs": [],
      "source": [
        "# CREAT JSON WITH THIS FORM TO DATA TRAINING\n",
        "\n",
        "x = {\"intents\":[\n",
        "    {\"tag\": \"saludos\",\n",
        "     \"patterns\": [\"hola\", \"hi\", \"hello\", \"kiubo\", \"que onda\", \"oli\",\n",
        "                  \"oliwis\", \"hey\", \"que pdo\", \"kiubole\", \"ola\", \"que tranza\"],\n",
        "     \"responses\": [\"Hola\", \"¡Qué tal!\", \"¡Qué onda!\"]\n",
        "     },\n",
        "    {\"tag\": \"adios\",\n",
        "     \"patterns\": [\"adios\", \"adiós\", \"bye\", \"ahí la ves\", \"ay la bimbo\", \n",
        "                  \"paro carnal\", \"hay nos wachamos\", \"nos vemos\", \"hasta luego\",\n",
        "                  \"hasta mañana\", \"buenas noches\", \"bonita tarde\"],\n",
        "     \"responses\": [\"¡Adiós, Hasta luego!\", \"¡Buen día! ¡Nos vemos!\", \n",
        "                   \"¡Qué le vaya bien!\", \"¡Hasta pronto wapx!\"]\n",
        "     }\n",
        "]}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LIBRARIES \n",
        "\n",
        "import random\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "8ZzsWUCF9En9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "O45q1R1MgrLn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df542b5a-fd4c-46ce-c0e1-0fa13a62d81f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AInPqvzCty8v",
        "outputId": "5b665305-b1a1-4872-c38e-fd62f62219a7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6BnouHRt9mc",
        "outputId": "88201044-75ff-45fd-bb7c-c925b82187b4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf \n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.optimizers import SGD"
      ],
      "metadata": {
        "id": "TOXXCE_ZgiVL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()  # initialization\n",
        "\n",
        "intents = json.loads(open('intents.json').read())\n",
        "# intetnts = json.loads(x)  # DIRECT DEFITION IN THE SAME CODE\n",
        "\n",
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_letters = [\"?\",\"¿\",\"¡\",\"!\",\",\",\".\",\";\",\":\",\"-\"]"
      ],
      "metadata": {
        "id": "7ZdSbcx8iA95"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for intent in intents['intents']:\n",
        "  for pattern in intent['patterns']:\n",
        "    word_list = nltk.word_tokenize(pattern)\n",
        "    words.extend(word_list)\n",
        "    documents.append((word_list, intent['tag']))\n",
        "    if intent['tag'] not in classes:\n",
        "      classes.append(intent['tag'])"
      ],
      "metadata": {
        "id": "sCcUf5-cmbHZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in ignore_letters]\n",
        "words = sorted(set(words))"
      ],
      "metadata": {
        "id": "hhgoc88opf24"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = sorted(set(classes))\n",
        "\n",
        "pickle.dump(words, open('words.pkl','wb'))\n",
        "pickle.dump(classes, open('classes.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "8GpkQjd0tQBv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training = []\n",
        "output_empty = [0] * len(classes)"
      ],
      "metadata": {
        "id": "tDxz_Z4BvLYl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for document in documents:\n",
        "  bag = []\n",
        "  word_patterns = document[0]\n",
        "  word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n",
        "  for word in words:\n",
        "    bag.append(1) if word in word_patterns else bag.append(0)\n",
        "  \n",
        "  output_row = list(output_empty)\n",
        "  output_row[classes.index(document[1])] = 1\n",
        "  training.append([bag, output_row])\n",
        "\n",
        "random.shuffle(training)\n",
        "training = np.array(training,dtype=object)\n",
        "\n",
        "train_x = list(training[:, 0])\n",
        "train_y = list(training[:, 1])"
      ],
      "metadata": {
        "id": "UBmTcfT-vkPL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape = (len(train_x[0]),), activation = 'relu'))\n",
        "model.add(Dropout(0.5))                     # Dropout(0,5) => Dropout(0.5)\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
        "\n",
        "sgd = SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)  # lr is deprecated\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "uUT3kHQzxk86"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist = model.fit(np.array(train_x), np.array(train_y), epochs=50, batch_size=5, verbose=1)\n",
        "model.save('chatbot_model.h5',hist)\n",
        "print('Done')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sFQ12QpsMZh",
        "outputId": "d7436c3c-5096-4674-a2f8-5dbe0948a72f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "5/5 [==============================] - 1s 4ms/step - loss: 0.7348 - accuracy: 0.5000\n",
            "Epoch 2/50\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.7234 - accuracy: 0.5000\n",
            "Epoch 3/50\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6901 - accuracy: 0.5833\n",
            "Epoch 4/50\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6685 - accuracy: 0.5417\n",
            "Epoch 5/50\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.6279 - accuracy: 0.7500\n",
            "Epoch 6/50\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6978 - accuracy: 0.4583\n",
            "Epoch 7/50\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.5958 - accuracy: 0.7917\n",
            "Epoch 8/50\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.6177 - accuracy: 0.6667\n",
            "Epoch 9/50\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.6198 - accuracy: 0.7083\n",
            "Epoch 10/50\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.5604 - accuracy: 0.9167\n",
            "Epoch 11/50\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.6112 - accuracy: 0.7500\n",
            "Epoch 12/50\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.5392 - accuracy: 0.8333\n",
            "Epoch 13/50\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.5000 - accuracy: 0.8333\n",
            "Epoch 14/50\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.4590 - accuracy: 0.9167\n",
            "Epoch 15/50\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.4548 - accuracy: 0.8333\n",
            "Epoch 16/50\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.4285 - accuracy: 0.9167\n",
            "Epoch 17/50\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.3624 - accuracy: 0.9583\n",
            "Epoch 18/50\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.3676 - accuracy: 0.8750\n",
            "Epoch 19/50\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.3080 - accuracy: 1.0000\n",
            "Epoch 20/50\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.3016 - accuracy: 0.8750\n",
            "Epoch 21/50\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.2804 - accuracy: 0.9167\n",
            "Epoch 22/50\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1956 - accuracy: 1.0000\n",
            "Epoch 23/50\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.2271 - accuracy: 0.9167\n",
            "Epoch 24/50\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1475 - accuracy: 1.0000\n",
            "Epoch 25/50\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.2037 - accuracy: 0.9167\n",
            "Epoch 26/50\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1588 - accuracy: 1.0000\n",
            "Epoch 27/50\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.1385 - accuracy: 1.0000\n",
            "Epoch 28/50\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0743 - accuracy: 1.0000\n",
            "Epoch 29/50\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0963 - accuracy: 1.0000\n",
            "Epoch 30/50\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0916 - accuracy: 1.0000\n",
            "Epoch 31/50\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.0942 - accuracy: 1.0000\n",
            "Epoch 32/50\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.0642 - accuracy: 1.0000\n",
            "Epoch 33/50\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.0606 - accuracy: 1.0000\n",
            "Epoch 34/50\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.1255 - accuracy: 0.9167\n",
            "Epoch 35/50\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0521 - accuracy: 1.0000\n",
            "Epoch 36/50\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0591 - accuracy: 1.0000\n",
            "Epoch 37/50\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0294 - accuracy: 1.0000\n",
            "Epoch 38/50\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.1165 - accuracy: 0.9583\n",
            "Epoch 39/50\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.0567 - accuracy: 0.9583\n",
            "Epoch 40/50\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.0344 - accuracy: 1.0000\n",
            "Epoch 41/50\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0665 - accuracy: 1.0000\n",
            "Epoch 42/50\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0306 - accuracy: 1.0000\n",
            "Epoch 43/50\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0727 - accuracy: 1.0000\n",
            "Epoch 44/50\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0260 - accuracy: 1.0000\n",
            "Epoch 45/50\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.0592 - accuracy: 1.0000\n",
            "Epoch 46/50\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0572 - accuracy: 1.0000\n",
            "Epoch 47/50\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0510 - accuracy: 1.0000\n",
            "Epoch 48/50\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0820 - accuracy: 0.9583\n",
            "Epoch 49/50\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.0104 - accuracy: 1.0000\n",
            "Epoch 50/50\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 0.0467 - accuracy: 1.0000\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model"
      ],
      "metadata": {
        "id": "IEFjLmjBw02Y"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = pickle.load(open('words.pkl','rb'))\n",
        "classes = pickle.load(open('classes.pkl','rb'))\n",
        "model = load_model('chatbot_model.h5')"
      ],
      "metadata": {
        "id": "-gyHeMdLxB-g"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_up_Sentence(sentence):\n",
        "  sentence_words = nltk.word_tokenize(sentence)\n",
        "  sentence_words = [lemmatizer.lemmatize(word) for word in sentence_words]\n",
        "  return sentence_words"
      ],
      "metadata": {
        "id": "NluWBnwfxrnm"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bag_of_words(sentence):\n",
        "  sentence_words = clean_up_Sentence(sentence)\n",
        "  bag = [0] * len(words)\n",
        "  for w in sentence_words:\n",
        "    for i, word in enumerate(words):\n",
        "      if word == w:\n",
        "        bag[i] = 1\n",
        "  return np.array(bag)"
      ],
      "metadata": {
        "id": "57TdC6u6yD9m"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_class(sentence):\n",
        "  bow = bag_of_words(sentence)\n",
        "  res = model.predict(np.array([bow]))[0]\n",
        "  ERROR_TRESHOLD = 0.25\n",
        "  results = [[i,r] for i,r in enumerate(res) if r > ERROR_TRESHOLD]\n",
        "  \n",
        "  results.sort(key=lambda x: x[1], reverse=True)\n",
        "  return_list = []\n",
        "  for r in results:\n",
        "    return_list.append({'intent':classes[r[0]], 'probability': str(r[1])})\n",
        "  return return_list"
      ],
      "metadata": {
        "id": "EtZUvQj-yhEk"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response(intents_list, intents_json):\n",
        "  tag = intents_list[0]['intent']\n",
        "  list_of_intents = intents_json['intents']\n",
        "  result = ''\n",
        "  for i in list_of_intents:\n",
        "    if i['tag'] == tag:\n",
        "      result = random.choice(i['responses'])\n",
        "      break\n",
        "  return result"
      ],
      "metadata": {
        "id": "ONdTQe_dzxs-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = \"\"\n",
        "print('Escribir 0 para finalizar \\n')\n",
        "\n",
        "while True:\n",
        "  message = input(\"Tú: \")\n",
        "  if message == '0': break\n",
        "  ints = predict_class(message)\n",
        "  res = get_response(ints, intents)\n",
        "  print(\"Bot: \",res)\n",
        "\n",
        "print(\"Bot: Gracias por hablar conmigo! >///<\")"
      ],
      "metadata": {
        "id": "M4c8iVfr0VEC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f28ceef3-e042-46f1-fff8-920e1a6182eb"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Escribir 0 para finalizar \n",
            "\n",
            "Tú: hi\n",
            "Bot:  ¡Qué onda!\n",
            "Tú: Hola\n",
            "Bot:  ¡Qué tal!\n",
            "Tú: que pdo\n",
            "Bot:  ¡Qué tal!\n",
            "Tú: adios\n",
            "Bot:  ¡Buen día! ¡Nos vemos!\n",
            "Tú: bye\n",
            "Bot:  ¡Buen día! ¡Nos vemos!\n",
            "Tú: 0\n",
            "Bot: Gracias por hablar conmigo! >///<\n"
          ]
        }
      ]
    }
  ]
}